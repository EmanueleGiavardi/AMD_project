{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODK9ON+i2MMH5ML7JODSzV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmanueleGiavardi/AMD_project/blob/main/reviews_link_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle pyspark"
      ],
      "metadata": {
        "id": "lVpPeMdRyhl-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import functions as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from google.colab import files\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "6c2uPSW2UgA8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# please upload your kaggle.json file here\n",
        "files.upload()\n",
        "!ls -lha kaggle.json\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "4HULTI5NxhlD",
        "outputId": "d2333ade-c576-4b08-c515-85794b8d88d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4af09ec5-61cd-4833-93c1-e87405d4d93c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4af09ec5-61cd-4833-93c1-e87405d4d93c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "-rw-r--r-- 1 root root 72 Jun 10 07:34 kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d \"mohamedbakhet/amazon-books-reviews\"\n",
        "!unzip amazon-books-reviews.zip\n",
        "!rm -r amazon-books-reviews.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DpRDvklyoWs",
        "outputId": "635bd7f3-2ed3-4ded-ae34-47242e53774f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
            "License(s): CC0-1.0\n",
            "Downloading amazon-books-reviews.zip to /content\n",
            " 99% 1.05G/1.06G [00:06<00:00, 159MB/s]\n",
            "100% 1.06G/1.06G [00:06<00:00, 184MB/s]\n",
            "Archive:  amazon-books-reviews.zip\n",
            "  inflating: Books_rating.csv        \n",
            "  inflating: books_data.csv          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()"
      ],
      "metadata": {
        "id": "vFGEUca4kg7B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = pyspark.sql.SparkSession.builder.master(\"local[*]\").appName(\"reviews_link_analysis\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "Zq7n-9Bo0BmL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books_rating_df = spark.read.csv(\"Books_rating.csv\", header=True, inferSchema=True)\n",
        "books_data_df = spark.read.csv(\"books_data.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "P4r1T4pQ9q8z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Link Analysis: finding influential/authoritative users**"
      ],
      "metadata": {
        "id": "7UNXVlWV6R4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset subsampling**"
      ],
      "metadata": {
        "id": "kBSi9hL4-RPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the original dataset, reviews are ordered by the book being reviewed. This means we have $K$ rows related to book $B$, followed by $K′$ rows for book $B′$, and so on.\n",
        "There are two main strategies for subsampling the dataset:\n",
        "\n",
        "1. **Selecting the first N rows without any shuffling**: \\\\\n",
        "This approach favors the inclusion of reviews for the same book, resulting in a smaller number of distinct books but a higher number of user-user connections.\n",
        "\n",
        "2. **Random sampling, where each row is included in the sample with probability p (using a random seed for reproducibility)**: \\\\\n",
        "This method leads to a broader variety of books in the sample, but it becomes less likely that two users have reviewed the same book, thus reducing the number of connected users.\n",
        "\n",
        "Although the second approach may be more statistically appropriate in terms of representative sampling, the first strategy is chosen for this project in order to ensure a denser graph with a more substantial number of user connections, since the goal of this project is to explore links between users who have reviewed the same book"
      ],
      "metadata": {
        "id": "OqgAjgvr8_qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_count = books_rating_df.count()\n",
        "\n",
        "# keeps the first (sampling_frac*100)% of the lines\n",
        "sampling_frac = 0.01\n",
        "books_rating_df_sub = books_rating_df.limit(int(sampling_frac * ratings_count))\n",
        "print(f\"sample has {books_rating_df_sub.count()} lines\")"
      ],
      "metadata": {
        "id": "vikm8jG7Csce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0755661d-afbf-45b1-8333-ced6d8782c22"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample has 30000 lines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Graph creation**"
      ],
      "metadata": {
        "id": "ybPI7GLK-LiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Nodes** represent users.\n",
        "- **Edges** represent links between users who have reviewed the same book.\n",
        "\n",
        "The graph is **directed**: a directed edge from $u2$ to $u1$ exists if both $u1$ and $u2$ reviewed the same book, and the **helpfulness score** of $u1$'s review is higher than that of $u2$'s review for the same book.\n",
        "\n",
        "We focus only on a subset of all available books. In the *books_data* CSV file, the ```categories``` column contains:\n",
        "\n",
        "- In most cases, a string representing a list of categories, e.g., ```\"['Religion', 'Politics', ...]\"```\n",
        "- In some cases, invalid or irrelevant data, such as ```None``` values or links to the Google Books Store\n",
        "\n",
        "To ensure the use of meaningful data, we only consider books for which the ```categories``` value matches the regular expression \\[.*\\], allowing us to parse it as an actual list of strings.\n",
        "\n",
        "This decision is motivated by the intent to apply **Topic-Sensitive PageRank** to the graph. In this context, the \"topic\" associated with each node (i.e., user) corresponds to their *preferred literary genre*, defined as the most frequently reviewed genre by that user.\n",
        "\n",
        "```\n",
        "# graph creation pseudocode:\n",
        "\n",
        "for each book b (with well-formatted categories):\n",
        "    for each (u1, u2) such that both u1 and u2 reviewed b:\n",
        "        if (helpfulness(u1, b)) > (helpfulness(u2, b)):\n",
        "            add edge from u2 to u1\n",
        "```\n",
        "\n",
        "\n",
        "---------------\n",
        "\n",
        "Given $R$, the review table in which the Title, User_id and helpfulness attributes have been selected, we construct the following new table:\n",
        "$$\n",
        "J = \\sigma_{\\text{helpfulness}_1 > \\text{helpfulness}_2}(R' \\Join_{\\text{Title}} R')\n",
        "$$\n",
        "\n",
        "This table has the schema\n",
        "```\n",
        "root\n",
        " |-- User_id_1\n",
        " |-- Title\n",
        " |-- User_id_2\n",
        " |-- Helpfulness_1\n",
        " |-- Helpfulness_2\n",
        "```\n",
        "It is constructed such that both ```User_id_1``` and ```User_id_2``` reviewd the book named with ```Title``` and ```Helpfulness_1``` $>$ ```Helpfulness_2```.\n",
        "\n",
        "From this table, we build the directed graph based on the criteria described above.\n",
        "\n",
        "\n",
        "**NOTE**:\n",
        "> The helpfulness scores of each review are not on a common scale as they appear in formats like \"0/0\", \"4/5\", \"8/10\", \"78/82\", and so on.\n",
        "For the purpose of this project, we simply convert each \"X/Y\" string into a float by evaluating the fraction.\n",
        "However, a finer interpretation would treat \"X/Y\" as \"number of people who found the review helpful / total number of voters\".\n",
        "Under this assumption, it's important to consider not only the fraction itself but also how many people voted, since a high ratio based on few votes may be less reliable than a slightly lower ratio based on many votes."
      ],
      "metadata": {
        "id": "t8WybkMtTsEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, col, when\n",
        "\n",
        "def get_helpfulness_score(col_name: pyspark.sql.Column):\n",
        "    \"\"\"\n",
        "    Returns the numerical helpfulness score associated with the ```\"X/Y\"``` string of the input column.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    col_name: pyspark.sql.Column\n",
        "        the name of the column\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    k: float\n",
        "        the numerical ratio associated with ```\"X/Y\"``` if Y is not \"0\", ```0.0``` otherwise\n",
        "    \"\"\"\n",
        "\n",
        "    num = split(col(col_name), \"/\").getItem(0).cast(\"float\")\n",
        "    den = split(col(col_name), \"/\").getItem(1).cast(\"float\")\n",
        "    return when(den != 0, num / den).otherwise(0.0)\n",
        "\n",
        "\n",
        "\n",
        "R = books_rating_df_sub.select([\"Title\", \"User_id\", \"review/helpfulness\"]).filter(col('User_id').isNotNull())\n",
        "R1 = R.alias(\"R1\")\n",
        "R2 = R.alias(\"R2\")\n",
        "\n",
        "# J schema: | Title | User_id_1 | User_id_2 | helpfulness_1 | helpfulness_2 |\n",
        "J = R1.join(R2, on=\"Title\") \\\n",
        "      .filter(col(\"R1.User_id\") != col(\"R2.User_id\")) \\\n",
        "      .select(\n",
        "          col(\"R1.Title\").alias(\"Title\"),\n",
        "          col(\"R1.User_id\").alias(\"User_id_1\"),\n",
        "          col(\"R2.User_id\").alias(\"User_id_2\"),\n",
        "          get_helpfulness_score(\"R1.review/helpfulness\").alias(\"helpfulness_1\"),\n",
        "          get_helpfulness_score(\"R2.review/helpfulness\").alias(\"helpfulness_2\")\n",
        "      ).filter(col(\"helpfulness_1\") > col(\"helpfulness_2\"))"
      ],
      "metadata": {
        "id": "oOoD6bekY-QV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding books with well-formatted \"categories\" value\n",
        "\n",
        "from pyspark.sql.functions import from_json\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "genres_schema = ArrayType(StringType())\n",
        "\n",
        "# output example: Row(Title='Wonderful Worship in Smaller Churches', Genres=['Religion', 'Politics']),\n",
        "genres = books_data_df.filter((col(\"categories\").isNotNull()) & (col(\"categories\").rlike(r\"^\\[.*\\]$\"))) \\\n",
        "        .withColumn(\"Genres\", from_json(\"categories\", genres_schema)).select(\"Title\", \"Genres\")\n",
        "\n",
        "# filtering J keeping only books with well-formatted categories\n",
        "# J_filtered schema: | Title | User_id_1 | User_id_2 | helpfulness_1 | helpfulness_2 | Genres\n",
        "J_filtered = J.join(genres, on=\"Title\")"
      ],
      "metadata": {
        "id": "KLPJM3JkTpVK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea now is to associate an **increasing integer value from $0$ to $N-1$** to each one of the $N$ user ids. In this way:\n",
        "- An edge is simply going to be represented as a couple of integers $(i, j)$, where $i$ is the integer value related to the user having the outgoing connection and $j$ us the integer value related to the user having the incoming connection\n",
        "- PageRank values will be stored in a simple array $V$ of $N$ elements, such that $V[i]$ = pageRank value for the user associated to integer value $i$\n",
        "\n"
      ],
      "metadata": {
        "id": "WlQL2g1PTy4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: (User_id_1 U User_id_2) excludes all the users that reviewd a certain book by themselves!\n",
        "\n",
        "unique_users = J_filtered.select(col(\"User_id_1\").alias(\"User_id\")) \\\n",
        "    .union(J_filtered.select(col(\"User_id_2\").alias(\"User_id\"))) \\\n",
        "    .distinct()\n",
        "N = unique_users.count()\n",
        "\n",
        "# [('user1_id', integer1), ... ('userN_id', integerN)]\n",
        "user_ids_rdd = unique_users.rdd.map(lambda row: row[\"User_id\"]).zipWithIndex()"
      ],
      "metadata": {
        "id": "a-SUQ2zq9fh-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now there could be two ways of creating the $(i, j)$ couples:\n",
        "1. from ```user_ids_rdd``` a Dataframe with schema ```[User_id, Integer_value]``` could be created, being able to associate each ```Integer_value``` both to ```User_id_1``` and ```User_id_2``` using join operations.\n",
        "2. convert ```user_ids_rdd``` in a dictionary which is broadcasted to every computing node of the cluster, so that becomes easy to retrieve the ```Integer_value``` extracting the value for the specific ```User_id``` key\n",
        "\n",
        "The number $N$ of distinct users is not expected to be _that high_, so we can assume that a dictionary containing $N$ entries can be held in main memory"
      ],
      "metadata": {
        "id": "A4lcUPbEW-KF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_ids_dict = user_ids_rdd.collectAsMap()\n",
        "user_ids_bdcast = sc.broadcast(user_ids_dict)"
      ],
      "metadata": {
        "id": "Jee4fKWYZT50"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of couples (node_src, node_dst)\n",
        "edges = J_filtered.rdd.map(lambda row : (user_ids_bdcast.value[row[2]], user_ids_bdcast.value[row[1]]))"
      ],
      "metadata": {
        "id": "sAzZ-FMaZzcO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of couples (node_src, [iterable of dst nodes])\n",
        "adjacency_list = edges.groupByKey()"
      ],
      "metadata": {
        "id": "0JUN88v9dDK0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(f\"Transition graph has {N} nodes and {edges.count()} edges\")"
      ],
      "metadata": {
        "id": "Fj5KhnqxLJEN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Graph Analysis**"
      ],
      "metadata": {
        "id": "pgYKAmwqegdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we look for Spider Traps (in the simplest form, i.e. 2 nodes) and Dead Ends in the graph"
      ],
      "metadata": {
        "id": "xmtpzCQvelir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dead ends\n",
        "dead_ends = adjacency_list.filter(lambda x: len(x[1]) == 0).count()\n",
        "print(f\"{dead_ends} dead ends have been found in this graph\")"
      ],
      "metadata": {
        "id": "ord2j_7veyrJ",
        "outputId": "fb44f980-a29d-45e1-8727-994c528ae307",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 dead ends have been found in this graph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import least, greatest\n",
        "\n",
        "# spider traps\n",
        "edges_df = edges.toDF([\"src\", \"dst\"])\n",
        "\n",
        "# tuples (u1, u2) such that u1 <==> u2\n",
        "mutual_edges = edges_df.alias(\"e1\") \\\n",
        "    .join(edges_df.alias(\"e2\"),\n",
        "          (col(\"e1.src\") == col(\"e2.dst\")) &\n",
        "          (col(\"e1.dst\") == col(\"e2.src\"))) \\\n",
        "    .select(\n",
        "        least(col(\"e1.src\"), col(\"e1.dst\")).alias(\"u1\"),\n",
        "        greatest(col(\"e1.src\"), col(\"e1.dst\")).alias(\"u2\")\n",
        "    ).distinct()"
      ],
      "metadata": {
        "id": "KYzZJfHhfLUN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_degrees = adjacency_list.map(lambda x: (x[0], len(x[1]))).toDF([\"node\", \"out_degree\"])\n",
        "\n",
        "# u1 | u2 | out_degree_1 | out_degree_2\n",
        "joined = mutual_edges \\\n",
        "    .join(out_degrees.withColumnRenamed(\"node\", \"u1\").withColumnRenamed(\"out_degree\", \"out_degree_1\"), on=\"u1\") \\\n",
        "    .join(out_degrees.withColumnRenamed(\"node\", \"u2\").withColumnRenamed(\"out_degree\", \"out_degree_2\"), on=\"u2\")\n",
        "\n",
        "# spider traps couples are those u1 <==> u2 such that\n",
        "# - u1 has only ONE outgoing edge (the one connecting it to u2)\n",
        "# - u2 has only ONE outgoing edge (the one connecting it to u1)\n",
        "# the number of spider traps couples in this context is expected to be small, so we can collect them in main memory\n",
        "spider_traps = joined.filter((col(\"out_degree_1\") == 1) & (col(\"out_degree_2\") == 1)).select(\"u1\", \"u2\").collect()"
      ],
      "metadata": {
        "id": "1WcZnA7wf1Lq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{len(spider_traps)} spider traps have been found in this graph: \")\n",
        "for t in spider_traps: print(f\"{t[0]} <==> {t[1]}\")"
      ],
      "metadata": {
        "id": "biwlsCKgX-q3",
        "outputId": "1a0b77a2-8926-4503-de8a-4c7095e13c43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 spider traps have been found in this graph: \n",
            "1605 <==> 3883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Topic vector creation**"
      ],
      "metadata": {
        "id": "tbKrWg_jdHjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic-Sensitive PageRank requires a topic assigned to each node of the graph. In this case each user is associated to the literary genre that he/she reviewed the most.\n",
        "The data structure that contains this information is an array such that the $i$-th element refers to the \"preferred\" literary genre for the user mapped to integer $i$"
      ],
      "metadata": {
        "id": "xaj7mx5gdSXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T1 = books_rating_df_sub.join(unique_users, \"User_id\").select(col(\"User_id\"), col(\"Title\")).alias(\"T1\")\n",
        "T2 = genres.alias(\"T2\")\n",
        "\n",
        "genres_per_review = T1.join(T2, on=\"Title\").select(\"User_id\", \"T1.Title\", \"Genres\")\n",
        "genres_per_review.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpYUb2u1a1pV",
        "outputId": "53f752aa-083a-460c-b8d8-75a5c161f83b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(User_id='A31WHFXF6T06DR', Title='Isaac Asimov: Master of Science Fiction (People to Know)', Genres=['Juvenile Nonfiction']),\n",
              " Row(User_id='A14B2NR2XELLQ0', Title='Isaac Asimov: Master of Science Fiction (People to Know)', Genres=['Juvenile Nonfiction']),\n",
              " Row(User_id='A7FAM0VNL7F4B', Title='Iridescent Soul', Genres=['Hummingbirds']),\n",
              " Row(User_id='A2YZWB84LMU5CC', Title='Iridescent Soul', Genres=['Hummingbirds']),\n",
              " Row(User_id='AUNJJ9J53PMGH', Title='Iridescent Soul', Genres=['Hummingbirds'])]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when setting the user as key and grouping by key we have something like:\n",
        "# ('A3U1XS6XK6YUU4' -> [['Fiction', 'Drama'], ['Religion'], ['Religion', 'Politics']]).\n",
        "# So the value is a nested list: Each internal list represents the genres of each book reviewd by the user\n",
        "\n",
        "# [genre for book_genres in external_list for genre in book_genres] just flattens the list:\n",
        "# same as:\n",
        "# for book_genres in external_list:\n",
        "#     for genre in book_genres:\n",
        "#         append genre to resulting list\n",
        "#\n",
        "# [['Fiction', 'Drama'], ['Religion'], ['Religion', 'Politics']] => ['Fiction', 'Drama', 'Religion', 'Religion', 'Politics']\n",
        "\n",
        "# the Counter simply returns the most common genre (in the example: 'Religion')\n",
        "# if the counting is the same for two distinct genres, the first one according to the alphabetical ordering is returned\n",
        "\n",
        "genres_per_user = genres_per_review.rdd.map(lambda row: (row['User_id'], row['Genres'])).groupByKey() \\\n",
        "    .mapValues(lambda external_list: [genre for book_genres in external_list for genre in book_genres]).mapValues(\n",
        "    lambda genres: Counter(genres).most_common(1)[0][0] if genres else None\n",
        ").map(lambda x: (user_ids_dict[x[0]], x[1]))\n",
        "\n",
        "genres_per_user_array = [None] * N\n",
        "\n",
        "# genres_per_user_array[i] = preferred genre for user mapped to integer i\n",
        "for index, genre in genres_per_user.collect(): genres_per_user_array[index] = genre"
      ],
      "metadata": {
        "id": "pDiejon-tU1Q"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "again, we're under the assumption that there's no problem in keeping a $N$ element array in main memory"
      ],
      "metadata": {
        "id": "QHXmsh91sN0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Topic-Sensitive PageRank**"
      ],
      "metadata": {
        "id": "6YWDBT5rnods"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all we need to initialize the transition matrix $M$ so that $M_{ij}$ = $\\frac{1}{α}$, where $\\alpha$ is the number of outgoing edges from node $j$ (if there's a link from $j$ to $i$).\n",
        "\n",
        "\n",
        "Now ```adjacency_list``` is a rdd in which each element is expressed ```(node, [neighbours])```. Since the transition matrix $M$ is heavily sparse, we are going to represent it using triplets $(i, j, M_{ij})$ only if $M_{ij} \\neq 0$\n",
        "\n",
        "**NOTE**: in this setting, we could potentially have many arcs from a certain node $A$ to another node $B$, because there could be many books for which user $B$ wrote a better review than user $A$.\n",
        "\n",
        "The idea in this case is to collapse all the possibile arcs from $A$ to $B$ in a single arc, weighting the associated pageRank initial value according to the actual number of books for which $B$ obtained a better score with respect to $A$.\n",
        "\n",
        "So triplets are actually stored in the form $((i, j), M_{ij})$, so that it becomes easy to group triples with the same key $(i, j)$ and summing up all the contributes $M_{ij}$ associated with the same src-dest nodes.\n",
        "\n",
        "_Example:_\n",
        "- ```edges = [(A, B), (A, B), (A, C)]```\n",
        "- ```adjacency_list = [A, [B, B, C]]```\n",
        "- ```\n",
        "triplets (before grouping) = [\n",
        "    ((B, A), 1/3)\n",
        "    ((B, A), 1/3)\n",
        "    ((C, A), 1/3)\n",
        "]\n",
        "```\n",
        "- ```\n",
        "triplets (after grouping) = [\n",
        "    ((B, A), 2/3)\n",
        "    ((C, A), 1/3)\n",
        "]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "9ZiuQzMfQyik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if el is an element of the adjacency_list rdd,\n",
        "# el[0] => node\n",
        "# el[1] => list of neighbours of that node\n",
        "\n",
        "# NOTE: now the semantics of the triplets is (i, j, Mij) => (dst_node, src_node, value)\n",
        "# it's a bit counterintuitive, but COLUMNS REPRESENT SOURCE NODES, while ROWS REPRESENT DESTINATION NODES\n",
        "\n",
        "triplets = adjacency_list.flatMap(\n",
        "    lambda el: [((neighbour, el[0]), 1.0/len(el[1])) for neighbour in el[1] if len(el[1]) > 0]\n",
        ").reduceByKey(lambda x, y: x + y)"
      ],
      "metadata": {
        "id": "bNF8vdaaSaQr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mapping back to canonical (i, j, Mij) form, for semplicity (and for coherence with lecture notes)\n",
        "M = triplets.map(lambda triplet: (triplet[0][0], triplet[0][1], triplet[1])).cache()"
      ],
      "metadata": {
        "id": "Ye3YvQlsTdTk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECK: M should be column-wise stochastic, so column values should sum up to 1\n",
        "# m[1] -> column index\n",
        "# m[2] -> initial pageRank score for i, j nodes\n",
        "check = M.map(lambda m: (m[1], m[2])).reduceByKey(lambda x, y: x+y)\n",
        "# now we have key-value pairs such that key => column index j and value = SUM(M[i,j]) for i = 0, ..., # rows - 1.\n",
        "# Check if some values are far from 1 (with a tolerance of epsilon)\n",
        "epsilon = 1e-6\n",
        "far_from_one = check.filter(lambda pair: abs(pair[1] - 1.0) > epsilon).count()\n",
        "if far_from_one == 0: print(f\"✅ M is column-wise stochastic\")\n",
        "else: print(f\"⚠️ M IS NOT column-wise stochastic: there are {far_from_one} columns that don't sum up to 1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwiAJ3xapYSw",
        "outputId": "0bf964a3-bbb6-43a3-e3c4-31de8275dc2b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ M is column-wise stochastic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic-Sensitive PageRank (with dumping factor β):**\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "    \\begin{cases}\n",
        "        v(0) = \\frac{1}{N}\\underline{1} \\\\\n",
        "        v(t+1) = \\beta Mv(t) + (1-\\beta)\\frac{e_S}{|S|}  \n",
        "    \\end{cases}\\,\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "where $S$ is a given literary genre (e.g. \"Fiction\"), |S| is the cardinality of the set of users interested to $S$ and $e_S$ is a vector such that\n",
        "$$\n",
        "\\begin{equation}\n",
        "    \\begin{cases}\n",
        "        e_S[i] == 1 & \\text{if user associated with integer value $i$ has $S$ as preferred genre} \\\\\n",
        "        e_S[i] == 0 & \\text{otherwise}\n",
        "    \\end{cases}\\,\n",
        "\\end{equation}\n",
        "$$"
      ],
      "metadata": {
        "id": "XD4_b0R0NPVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_topic_vector(N: int, genres_per_user_array: np.ndarray, topic: str):\n",
        "    \"\"\"\n",
        "    Creates the vector used for taxation in Topic-Sensitive Page Rank.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    N: int\n",
        "        the number of distinct users\n",
        "    genres_per_user_array: np.ndarray\n",
        "        a vector containing the \"preferred\" literary genre for each user\n",
        "    topic: str\n",
        "        the chosen topic for PageRank computation\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    a vector containing ```1/N``` for each user if ```N``` is ```None```,\n",
        "    ```e_s / |S|``` otherwise\n",
        "    \"\"\"\n",
        "    if topic is None: return np.ones(N) / N\n",
        "    e_S = np.zeros(N)\n",
        "    for index, genre in enumerate(genres_per_user_array):\n",
        "        if genre == topic: e_S[index] = 1\n",
        "\n",
        "    # sanity check: the number of occurrences of topic in genres_per_user_dict.values() (which is |S|) should be equal to the number of ones in e_S\n",
        "    S_card = Counter(genres_per_user_array)[topic]\n",
        "    if not S_card == int(sum(e_S)): raise(ValueError(\"ERROR: something went wrong during topic vector creation\"))\n",
        "    return e_S / S_card"
      ],
      "metadata": {
        "id": "7zS8EhSj1Nd-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PageRank(M: pyspark.rdd.PipelinedRDD, v: np.ndarray, topic_vector: np.ndarray, max_iterations=100, tolerance=10e-5, beta=0.8):\n",
        "    \"\"\"\n",
        "    Computed the PageRank score for the graph described by ```M```\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    M: pyspark.rdd.PipelinedRDD\n",
        "        an rdd containing triplets (i, j, M_ij) describing the transition matrix\n",
        "    v: np.ndarray\n",
        "        a vector containing PageRank scores for each user\n",
        "    topic_vector: np.ndarray:\n",
        "        a vector containing the taxation probability for each user (uniform for non-topic sensitive PageRank)\n",
        "    max_iterations: int\n",
        "        the maximum number of iterations allowed\n",
        "    tolerance: float\n",
        "        the threshold under which the difference between the previews PageRank vector and the current PageRank vector is irrelevant\n",
        "    beta: float\n",
        "        the taxation parameter\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    v: np.ndarray\n",
        "        a vector containing PageRank scores for each user\n",
        "    \"\"\"\n",
        "    iteration = 0\n",
        "    while iteration < max_iterations:\n",
        "        prev_v = v.copy()\n",
        "\n",
        "        # broadcast v to each node of the cluster\n",
        "        v_bdcast = sc.broadcast(v)\n",
        "\n",
        "        # matrix - vector multiplication (distributed)\n",
        "        pr_scores = M.map(lambda m: (m[0], m[2]*v_bdcast.value[m[1]])).reduceByKey(lambda x, y: x + y).collect()\n",
        "        # update vector v (local)\n",
        "        for (user, pr_score) in pr_scores: v[user] = beta * pr_score + (1 - beta) * topic_vector[user]\n",
        "\n",
        "        dist = np.linalg.norm(v - prev_v)\n",
        "\n",
        "        if dist < tolerance:\n",
        "            print(f\"Convergence reached after {iteration} iterations with distance {dist}\")\n",
        "            break\n",
        "\n",
        "        print(f\"iteration {iteration}: distance = {dist}\")\n",
        "        iteration += 1\n",
        "    return v"
      ],
      "metadata": {
        "id": "tvaE9eQ206Kz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just to show some topics:\n",
        "\n",
        "Counter(genres_per_user_array).most_common(30)"
      ],
      "metadata": {
        "id": "_Jn5lZ4pMuTK",
        "outputId": "1b46dbfa-0019-4944-e541-d7e26ed83c25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Fiction', 5648),\n",
              " ('Book burning', 1065),\n",
              " ('Business & Economics', 575),\n",
              " ('Education', 564),\n",
              " ('Biography & Autobiography', 487),\n",
              " ('Religion', 471),\n",
              " ('American fiction', 454),\n",
              " ('History', 367),\n",
              " ('True Crime', 350),\n",
              " ('Body, Mind & Spirit', 320),\n",
              " ('Health & Fitness', 310),\n",
              " ('Juvenile Fiction', 291),\n",
              " ('Cooking', 207),\n",
              " ('Copyright', 199),\n",
              " ('Sports & Recreation', 189),\n",
              " ('Study Aids', 187),\n",
              " ('Family & Relationships', 177),\n",
              " ('Poetry', 176),\n",
              " ('Computers', 157),\n",
              " ('Philosophy', 125),\n",
              " ('Crafts & Hobbies', 119),\n",
              " ('Language Arts & Disciplines', 117),\n",
              " ('Psychology', 113),\n",
              " ('Travel', 90),\n",
              " ('Music', 89),\n",
              " ('Juvenile Nonfiction', 89),\n",
              " ('Science', 87),\n",
              " ('England', 85),\n",
              " ('Games', 77),\n",
              " ('Self-Help', 73)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for non-topic sensitive pageRank computation, just set topic to None\n",
        "topic = None\n",
        "\n",
        "if topic != None and topic not in genres_per_user_array: raise(AttributeError(\"Error: please select a valid literary genre\"))\n",
        "\n",
        "v = np.ones(N) / N\n",
        "topic_vector = create_topic_vector(N, genres_per_user_array, topic)\n",
        "\n",
        "max_iterations = 100\n",
        "tolerance = 10e-5\n",
        "beta = 0.8"
      ],
      "metadata": {
        "id": "ZscVx5UVG41o"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "pg_scores = PageRank(M, v, topic_vector, max_iterations=max_iterations, tolerance=tolerance, beta=beta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMEdVy7kRfJi",
        "outputId": "2f6ec9dc-d9ac-4822-8761-c09f716a625b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0: distance = 0.004508294288083763\n",
            "iteration 1: distance = 0.001392548552744607\n",
            "iteration 2: distance = 0.0005269557574718849\n",
            "iteration 3: distance = 0.00022173774433604593\n",
            "iteration 4: distance = 0.00011146929390574273\n",
            "Convergence reached after 5 iterations with distance 5.9924181026157786e-05\n",
            "CPU times: user 650 ms, sys: 24.4 ms, total: 675 ms\n",
            "Wall time: 28.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finding profileNames for the most k authoritative users\n",
        "\n",
        "k = 5\n",
        "pagerank_top_k_users = np.argsort(pg_scores)[-k:][::-1]\n",
        "\n",
        "# reversing user_ids_dict in an array\n",
        "user_ids_array = [None] * N\n",
        "for id, i in user_ids_dict.items(): user_ids_array[i] = id\n",
        "\n",
        "# here we're collecting in main memory a dict with only k elements (User_id -> profileName)\n",
        "profileNames = books_rating_df_sub.filter(books_rating_df_sub.User_id.isin([user_ids_array[user] for user in pagerank_top_k_users]))  \\\n",
        "    .select('User_id', 'profileName').distinct().rdd.collectAsMap()\n",
        "\n",
        "print(f\"PageRank top {k} users [topic: {topic}]\")\n",
        "for user in pagerank_top_k_users: print(f\"username: {profileNames[user_ids_array[user]]} -> PageRank score: {pg_scores[user]}\")"
      ],
      "metadata": {
        "id": "zlBBvcMFlxWz",
        "outputId": "affb4488-f8b5-44dc-e8de-5dc922275ae5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PageRank top 5 users [topic: None]\n",
            "username: Midwest Book Review -> PageRank score: 0.0013815026485379056\n",
            "username: \"E. A Solinas \"\"ea_solinas\"\"\" -> PageRank score: 0.0008588802397190114\n",
            "username: \"Terri J. Rice \"\"ricepaper\"\"\" -> PageRank score: 0.0008068922060201434\n",
            "username: Harriet Klausner -> PageRank score: 0.0006907966501004307\n",
            "username: \"booksforabuck \"\"BooksForABuck\"\"\" -> PageRank score: 0.0005986556961684647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With ```Topic = None``` the most influential/authoritative user appears to be \"[Midwest Book Review](https://www.midwestbookreview.com/)\", which is a fairly well-known organization focused on book reviews. So, in this example, the PageRank result seems quite reasonable.\n",
        "\n"
      ],
      "metadata": {
        "id": "kq5ejl5QNT95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "end_time = time.time()\n",
        "elapsed_time = (end_time - start_time) / 60\n",
        "print(f\"Global runtime: {elapsed_time:.2f} minutes\")"
      ],
      "metadata": {
        "id": "e3wQQo1Eksau",
        "outputId": "426fcd6f-81e0-4c1f-df49-d4aef7f9ba1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global runtime: 10.44 minutes\n"
          ]
        }
      ]
    }
  ]
}